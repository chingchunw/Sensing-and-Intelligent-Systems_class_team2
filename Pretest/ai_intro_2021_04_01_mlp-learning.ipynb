{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lIUGR61n6291"
   },
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then run the cells accordingly.\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "77NOrffR6293"
   },
   "outputs": [],
   "source": [
    "NAME = \"\"\n",
    "COLLABORATORS = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tz446rf06294"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lP01xK-AFBvn"
   },
   "source": [
    "#Introduction to AI - Robotics\n",
    "\n",
    "This is an undergrad course in NYCU.\n",
    "\n",
    "This tutorial includes two parts:\n",
    "* The MLP is from the BMM summer course 2018.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WjDlrPNkDWAO"
   },
   "source": [
    "#Multi-Layer Perceptron\n",
    "\n",
    "**Reference** Yen-Ling Kuo and Eugenio Piasini for the [Brains, Minds and Machines summer course 2018](http://cbmm.mit.edu/summer-school/2018).\n",
    "\n",
    "## Implementing a Multi-layer Perceptron\n",
    "\n",
    "### XOr Problem\n",
    "\n",
    "The XOr (exclusive or) problem is a classical problem in artificial neural network research. It uses a neural network to predict the output of a XOr logic gate given two binary inputs.\n",
    "\n",
    "Here, we relax the XOr problem a bit. Instead of binary inputs, we give inputs in real values. When the two input values have the same sign, the network should output 0; otherwise, the network should output 1.\n",
    "\n",
    "This is how the training data looks like in a 2D plot. The red dot means the output label is 0, the blue dot means the output label is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "executionInfo": {
     "elapsed": 2386,
     "status": "ok",
     "timestamp": 1603694993124,
     "user": {
      "displayName": "啟安李",
      "photoUrl": "",
      "userId": "07103444088692880013"
     },
     "user_tz": -480
    },
    "id": "mNys4JIqDy5c",
    "outputId": "3e075ba7-7b8c-43f8-a926-b60c1ad7d345"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# input of the MLP\n",
    "X = np.array([[1,1], [1,-1], [-1,-1], [-1,1]])\n",
    "# output of the MLP\n",
    "y = np.array([[0], [1], [0], [1]])\n",
    "\n",
    "# plot the training data\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(y.shape[0]):\n",
    "  if y[i][0] == 0:\n",
    "    marker = 'ro'\n",
    "  else:\n",
    "    marker = 'bo'\n",
    "  ax.plot(X[i][0], X[i][1], marker)\n",
    "ax.axhline(y=0, color='k')\n",
    "ax.axvline(x=0, color='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1_dUoJa0EJzc"
   },
   "source": [
    "Clearly, these data points are not linearly separable. We can not find one single line to classify the points correctly.\n",
    "\n",
    "In this part, we will build a multi-layer perceptron with nonlinearity to make predictions.\n",
    "\n",
    "### Implementing Activation Functions\n",
    "\n",
    "There are several activation functions to choose from, including sigmoid, tanh, relu, etc. Here, we implemented the sigmoid function as an example. In order to do backpropagation, we need to implement its derivative as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wJKFbAk4Ju4J"
   },
   "source": [
    "Please implement the sigmoid function:\n",
    "$$\\sigma(x) = \\frac{1}{1+e^{-x}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "kVd5Nwg3EFFZ",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9fa4d0126c7924b277aba5ff6d863650",
     "grade": false,
     "grade_id": "cell-586351873386a7e2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Sigmoid Function\n",
    "def sigmoid(x):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "q9F1B8sAjKsL",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9806c335743db7415320f939182a7277",
     "grade": true,
     "grade_id": "cell-f420ed6a31c95907",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert sigmoid(100) == 1\n",
    "assert sigmoid(-100) < 0.000000001\n",
    "assert sigmoid(1.2) - 0.768524783 < 0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s2dUv3fuJu4K",
    "outputId": "1490740e-c18f-4aa2-92fa-e58309b8a587"
   },
   "outputs": [],
   "source": [
    "x_axis = np.linspace(-10, 10, 100)\n",
    "y_axis = sigmoid(x_axis)\n",
    "\n",
    "  \n",
    "plt.plot(x_axis, y_axis)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"Sigmoid(X)\")\n",
    "  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9wDFNeQUJu4K"
   },
   "source": [
    "\n",
    "For more detail of derivative of sigmoid function:\n",
    "https://towardsdatascience.com/derivative-of-the-sigmoid-function-536880cf918e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VSP4Qo4EJu4K"
   },
   "outputs": [],
   "source": [
    "# Derivative of Sigmoid Function\n",
    "def derivative_sigmoid(x):\n",
    "    return x * (1 - x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AMSMkEMeEZBI"
   },
   "source": [
    "### A Simple MLP Architecture\n",
    "\n",
    "We design a simple MLP to solve this problem. The MLP consists of a hidden layer and an output layer to map the hidden vector to the output values. \n",
    "\n",
    "First, we initialize the weight and bias of each layer as well as the training epoch and learning rate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "egzciR4GEbo6"
   },
   "outputs": [],
   "source": [
    "epoch = 10000 # number of training iterations\n",
    "learning_rate = 0.1\n",
    "\n",
    "# dimension of each layer\n",
    "d_in = X.shape[1] # number of features in the input dataset\n",
    "d_h = 3   # hidden layer\n",
    "d_out = 1 # output layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3rW-P7aZEf3e"
   },
   "source": [
    "For each training iteration, we run one forward pass to get the predicted value and compute the loss between the prediction and the real value. For simplicity, we use the differences between the two values as the loss function. Then, we can compute the gradients and finally update the weights and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "2FKv95B4Ekoe",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3dbc300b99835b386f0de39d8f1abb0b",
     "grade": false,
     "grade_id": "cell-1d0b8ed8b2cc93b7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def train_mlp(d_in, d_h, d_out, epoch = 10000, learning_rate = 0.1):\n",
    "\n",
    "    # weight and bias initialization\n",
    "    wh = np.random.uniform(size=(d_in, d_h))\n",
    "    bh = np.random.uniform(size=(1, d_h))\n",
    "    wout = np.random.uniform(size=(d_h, d_out))\n",
    "    bout = np.random.uniform(size=(1, d_out))\n",
    "    loss_epoch = []\n",
    "\n",
    "    for i in range(epoch):\n",
    "        # Forward pass\n",
    "        h = sigmoid(X.dot(wh) + bh)\n",
    "        y_pred = sigmoid(h.dot(wout) + bout)\n",
    "\n",
    "        # Compute and print loss\n",
    "        loss = (y_pred - y).sum()\n",
    "        loss_epoch.append(loss)\n",
    "\n",
    "        if i % 500 == 0:\n",
    "            print('Epoch', i, ':', loss)\n",
    "\n",
    "        # Backpropagation to compute gradients\n",
    "        grad_y_pred = (y - y_pred) * derivative_sigmoid(y_pred)\n",
    "        grad_wout = h.T.dot(grad_y_pred)\n",
    "        grad_bout = np.sum(grad_y_pred, axis=0, keepdims=True)\n",
    "        grad_h = grad_y_pred.dot(wout.T) * derivative_sigmoid(h)\n",
    "        grad_wh = X.T.dot(grad_h)\n",
    "        grad_bh = np.sum(grad_h, axis=0, keepdims=True)\n",
    "\n",
    "        # Update weights and biases\n",
    "        # hint:\n",
    "        # you should update wout, bout, wh, bh with grad_wout, grad_bout, grad_wh, grad_bh multiplied by learning_rate\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    return loss_epoch, loss, y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "fg5qTocfJu4M",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d1597d4d35cc199a3bb8ea2f0ecfdd61",
     "grade": true,
     "grade_id": "cell-8adc1ab14ba9d1b0",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "e351622b-81d6-4cba-8f2e-16035e7d81a2"
   },
   "outputs": [],
   "source": [
    "loss_epoch, loss, y_pred = train_mlp(d_in, d_h, d_out, epoch, learning_rate)\n",
    "assert loss < 0.1\n",
    "\n",
    "x_axis = list(range(epoch))\n",
    "y_axis = loss_epoch\n",
    "plt.plot(x_axis, y_axis)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "TJ_AcLKkJu4M",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "545a7730428e93ed90edd0ece0df6998",
     "grade": true,
     "grade_id": "cell-e927e1d6d664bcc5",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "c8e1d857-e193-48a8-f5fa-5cd6f3809285"
   },
   "outputs": [],
   "source": [
    "loss_epoch, loss, y_pred = train_mlp(d_in, d_h, d_out, epoch, learning_rate)\n",
    "print('Prediction of training data:')\n",
    "print(y_pred)\n",
    "\n",
    "assert y_pred[0] < 0.1\n",
    "assert y_pred[1] > 0.9\n",
    "assert y_pred[2] < 0.1\n",
    "assert y_pred[3] > 0.9\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jlZA3tPbEvMI"
   },
   "source": [
    "The predicted values are very close to the labels in the training data. Since they are real values, to get the binary output, we can simply threshold the output to decide which label we want to assign.\n",
    "\n",
    "Although it is possible to implement any kind of layers and optmization methods with numpy, there are several machine learning libraries available. So, it is easier to build neural networks of different architectures.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ai_intro_2021_04_01_mlp-learning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
